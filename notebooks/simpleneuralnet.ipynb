{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import keras\n",
    "import keras.preprocessing.text as kpt\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "seed = 123\n",
    "max_words = 5000 # only work with the 5000 most popular words found in our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(964, 7)\n",
      "(752, 6)\n",
      "X Shape: (1716,)\n",
      "y shape: (1716, 1)\n"
     ]
    }
   ],
   "source": [
    "positive_df = pd.read_csv(\"radiocanada_small.csv\", dtype='str')\n",
    "negative_df = pd.read_csv(\"fakenews.csv\", dtype='str')\n",
    "print(positive_df.shape)\n",
    "print(negative_df.shape)\n",
    "\n",
    "X_positive = positive_df[\"text\"]\n",
    "y_positive = pd.DataFrame(np.ones(positive_df.shape[0]))\n",
    "\n",
    "X_negative = negative_df[\"text\"]\n",
    "y_negative = pd.DataFrame(np.zeros(negative_df.shape[0]))\n",
    "\n",
    "train_x = pd.concat([X_positive, X_negative], axis=0)\n",
    "train_y = pd.concat([y_positive, y_negative], axis=0)\n",
    "\n",
    "train_x = train_x.as_matrix().astype('str')\n",
    "\n",
    "print(\"X Shape:\", train_x.shape)\n",
    "print(\"y shape:\", train_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a new Tokenizer\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "# feed our tweets to the Tokenizer\n",
    "tokenizer.fit_on_texts(train_x)\n",
    "\n",
    "# Tokenizers come with a convenient list of words and IDs\n",
    "dictionary = tokenizer.word_index\n",
    "# Let's save this out so we can use it later\n",
    "with open('dictionary.json', 'w') as dictionary_file:\n",
    "    json.dump(dictionary, dictionary_file)\n",
    "\n",
    "\n",
    "def convert_text_to_index_array(text):\n",
    "    # one really important thing that `text_to_word_sequence` does\n",
    "    # is make all texts the same length -- in this case, the length\n",
    "    # of the longest text in the set.\n",
    "    return [dictionary[word] for word in kpt.text_to_word_sequence(text)]\n",
    "\n",
    "allWordIndices = []\n",
    "# for each tweet, change each token to its ID in the Tokenizer's word_index\n",
    "for text in train_x:\n",
    "    wordIndices = convert_text_to_index_array(text)\n",
    "    allWordIndices.append(wordIndices)\n",
    "\n",
    "# now we have a list of all tweets converted to index arrays.\n",
    "# cast as an array for future usage.\n",
    "allWordIndices = np.asarray(allWordIndices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create one-hot matrices out of the indexed tweets\n",
    "train_x = tokenizer.sequences_to_matrix(allWordIndices, mode='binary')\n",
    "# treat the labels as categories\n",
    "train_y = keras.utils.to_categorical(train_y, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(512, input_shape=(max_words,), activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(256, activation='sigmoid'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(2, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "  optimizer='adam',\n",
    "  metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1544 samples, validate on 172 samples\n",
      "Epoch 1/10\n",
      "1544/1544 [==============================] - 2s 1ms/step - loss: 0.2847 - acc: 0.8711 - val_loss: 0.0707 - val_acc: 0.9942\n",
      "Epoch 2/10\n",
      "1544/1544 [==============================] - 1s 827us/step - loss: 0.0292 - acc: 0.9935 - val_loss: 0.0287 - val_acc: 0.9942\n",
      "Epoch 3/10\n",
      "1544/1544 [==============================] - 1s 862us/step - loss: 0.0093 - acc: 0.9981 - val_loss: 0.0230 - val_acc: 0.9942\n",
      "Epoch 4/10\n",
      "1544/1544 [==============================] - 1s 839us/step - loss: 0.0065 - acc: 0.9994 - val_loss: 0.0186 - val_acc: 0.9942\n",
      "Epoch 5/10\n",
      "1544/1544 [==============================] - 2s 1ms/step - loss: 0.0043 - acc: 0.9994 - val_loss: 0.0160 - val_acc: 0.9942\n",
      "Epoch 6/10\n",
      "1544/1544 [==============================] - 1s 891us/step - loss: 0.0016 - acc: 1.0000 - val_loss: 0.0166 - val_acc: 0.9942\n",
      "Epoch 7/10\n",
      "1544/1544 [==============================] - 1s 818us/step - loss: 0.0015 - acc: 1.0000 - val_loss: 0.0180 - val_acc: 0.9942\n",
      "Epoch 8/10\n",
      "1544/1544 [==============================] - 1s 841us/step - loss: 0.0012 - acc: 1.0000 - val_loss: 0.0195 - val_acc: 0.9942\n",
      "Epoch 9/10\n",
      "1544/1544 [==============================] - 1s 833us/step - loss: 0.0011 - acc: 1.0000 - val_loss: 0.0217 - val_acc: 0.9942\n",
      "Epoch 10/10\n",
      "1544/1544 [==============================] - 1s 895us/step - loss: 5.1879e-04 - acc: 1.0000 - val_loss: 0.0211 - val_acc: 0.9942\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7efe3cd1f748>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_x, train_y,\n",
    "  batch_size=32,\n",
    "  epochs=10,\n",
    "  verbose=1,\n",
    "  validation_split=0.1,\n",
    "  shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save model\n",
    "\n",
    "model_json = model.to_json()\n",
    "with open('model.json', 'w') as json_file:\n",
    "    json_file.write(model_json)\n",
    "\n",
    "model.save_weights('model.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py35]",
   "language": "python",
   "name": "conda-env-py35-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

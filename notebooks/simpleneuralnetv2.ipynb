{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network trained on more data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import keras\n",
    "import keras.preprocessing.text as kpt\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "seed = 123\n",
    "max_words = 5000 # only work with the 5000 most popular words found in our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2436, 7)\n",
      "(2436, 6)\n",
      "X Shape: (4872,)\n",
      "y shape: (4872, 1)\n"
     ]
    }
   ],
   "source": [
    "negative_df = pd.read_csv(\"fakenews_big.csv\", dtype='str')\n",
    "# negative_df2 = pd.read_csv(\"fakenews_big2.csv\", dtype='str')\n",
    "\n",
    "positive_df = pd.read_csv(\"radiocanada.csv\", dtype='str')\n",
    "positive_df.columns = positive_df.ix[0]\n",
    "positive_df = positive_df[1:]\n",
    "positive_df = positive_df.sample(n=(negative_df.shape[0]), random_state=seed, axis=0)\n",
    "\n",
    "print(positive_df.shape)\n",
    "print(negative_df.shape)\n",
    "# print(negative_df2.shape)\n",
    "\n",
    "\n",
    "X_positive = positive_df[\"text\"]\n",
    "y_positive = pd.DataFrame(np.ones(positive_df.shape[0]))\n",
    "\n",
    "X_negative = negative_df[\"text\"]\n",
    "y_negative = pd.DataFrame(np.zeros(negative_df.shape[0]))\n",
    "\n",
    "# X_negative2 = negative_df2[\"text\"]\n",
    "# y_negative2 = pd.DataFrame(np.zeros(negative_df2.shape[0]))\n",
    "\n",
    "\n",
    "train_x = pd.concat([X_positive, X_negative], axis=0)\n",
    "train_y = pd.concat([y_positive, y_negative], axis=0)\n",
    "\n",
    "train_x = train_x.as_matrix().astype('str')\n",
    "\n",
    "print(\"X Shape:\", train_x.shape)\n",
    "print(\"y shape:\", train_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a new Tokenizer\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "# feed our tweets to the Tokenizer\n",
    "tokenizer.fit_on_texts(train_x)\n",
    "\n",
    "# Tokenizers come with a convenient list of words and IDs\n",
    "dictionary = tokenizer.word_index\n",
    "# Let's save this out so we can use it later\n",
    "with open('dictionary2.json', 'w') as dictionary_file:\n",
    "    json.dump(dictionary, dictionary_file)\n",
    "\n",
    "\n",
    "def convert_text_to_index_array(text):\n",
    "    # one really important thing that `text_to_word_sequence` does\n",
    "    # is make all texts the same length -- in this case, the length\n",
    "    # of the longest text in the set.\n",
    "    return [dictionary[word] for word in kpt.text_to_word_sequence(text)]\n",
    "\n",
    "allWordIndices = []\n",
    "# for each tweet, change each token to its ID in the Tokenizer's word_index\n",
    "for text in train_x:\n",
    "    wordIndices = convert_text_to_index_array(text)\n",
    "    allWordIndices.append(wordIndices)\n",
    "\n",
    "# now we have a list of all tweets converted to index arrays.\n",
    "# cast as an array for future usage.\n",
    "allWordIndices = np.asarray(allWordIndices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "198"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(allWordIndices[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create one-hot matrices out of the indexed tweets\n",
    "train_x = tokenizer.sequences_to_matrix(allWordIndices, mode='binary')\n",
    "# treat the labels as categories\n",
    "train_y = keras.utils.to_categorical(train_y, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(512, input_shape=(max_words,), activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(256, activation='sigmoid'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(2, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "  optimizer='adam',\n",
    "  metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 59078 samples, validate on 6565 samples\n",
      "Epoch 1/10\n",
      "59078/59078 [==============================] - 53s 899us/step - loss: 6.9127e-04 - acc: 0.9998 - val_loss: 5.6327 - val_acc: 0.6289\n",
      "Epoch 2/10\n",
      "59078/59078 [==============================] - 53s 895us/step - loss: 1.4687e-06 - acc: 1.0000 - val_loss: 5.7882 - val_acc: 0.6289\n",
      "Epoch 3/10\n",
      "59078/59078 [==============================] - 52s 887us/step - loss: 6.4790e-07 - acc: 1.0000 - val_loss: 5.8748 - val_acc: 0.6289\n",
      "Epoch 4/10\n",
      "59078/59078 [==============================] - 53s 896us/step - loss: 2.9951e-07 - acc: 1.0000 - val_loss: 5.9284 - val_acc: 0.6289\n",
      "Epoch 5/10\n",
      "59078/59078 [==============================] - 53s 890us/step - loss: 1.7224e-07 - acc: 1.0000 - val_loss: 5.9494 - val_acc: 0.6289\n",
      "Epoch 6/10\n",
      "59078/59078 [==============================] - 53s 892us/step - loss: 1.5389e-07 - acc: 1.0000 - val_loss: 5.9676 - val_acc: 0.6289\n",
      "Epoch 7/10\n",
      "59078/59078 [==============================] - 53s 895us/step - loss: 1.3183e-07 - acc: 1.0000 - val_loss: 5.9749 - val_acc: 0.6289\n",
      "Epoch 8/10\n",
      "59078/59078 [==============================] - 53s 902us/step - loss: 1.2573e-07 - acc: 1.0000 - val_loss: 5.9787 - val_acc: 0.6289\n",
      "Epoch 9/10\n",
      "59078/59078 [==============================] - 53s 889us/step - loss: 1.2180e-07 - acc: 1.0000 - val_loss: 5.9800 - val_acc: 0.6289\n",
      "Epoch 10/10\n",
      "59078/59078 [==============================] - 53s 900us/step - loss: 1.2086e-07 - acc: 1.0000 - val_loss: 5.9806 - val_acc: 0.6289\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fa271a25a90>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_x, train_y,\n",
    "  batch_size=32,\n",
    "  epochs=10,\n",
    "  verbose=1,\n",
    "  validation_split=0.1,\n",
    "  shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save model\n",
    "\n",
    "model_json = model.to_json()\n",
    "with open('model2.json', 'w') as json_file:\n",
    "    json_file.write(model_json)\n",
    "\n",
    "model.save_weights('model2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py35]",
   "language": "python",
   "name": "conda-env-py35-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
